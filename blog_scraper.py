import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urlparse
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import time

class BlogScraper:
    def __init__(self, use_selenium=True):
        self.use_selenium = use_selenium
        
        # ì¼ë°˜ requests ì„¸ì…˜
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # Selenium ì›¹ë“œë¼ì´ë²„ (í•„ìš”ì‹œ)
        self.driver = None
        if use_selenium:
            self._init_selenium_driver()
    
    def _init_selenium_driver(self):
        """Selenium ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¸°ê¸°
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            print("âœ… Selenium ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ Selenium ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("ì¼ë°˜ ìŠ¤í¬ë˜í•‘ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            self.use_selenium = False
    
    def scrape_naver_blog_selenium(self, url):
        """Seleniumì„ ì‚¬ìš©í•œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í•‘"""
        if not self.driver:
            return {'error': 'Selenium ë“œë¼ì´ë²„ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}
        
        try:
            print("ğŸ”„ Seleniumìœ¼ë¡œ í˜ì´ì§€ ë¡œë”© ì¤‘...")
            self.driver.get(url)
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            time.sleep(3)
            
            # JavaScript ì‹¤í–‰ ì™„ë£Œ ëŒ€ê¸°
            WebDriverWait(self.driver, 10).until(
                lambda driver: driver.execute_script("return document.readyState") == "complete"
            )
            
            # ì¶”ê°€ ëŒ€ê¸° (ë™ì  ì½˜í…ì¸  ë¡œë”©)
            time.sleep(2)
            
            # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ (ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„)
            title = ""
            title_selectors = [
                'h3.se_title',
                '.se-title-text', 
                '.se_title',
                '.pcol1 .se_title',
                '.title_area .title',
                'h1', 'h2', 'h3'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem and title_elem.get_text().strip():
                    title = title_elem.get_text().strip()
                    break
            
            # ë³¸ë¬¸ ì¶”ì¶œ (ë” ê´‘ë²”ìœ„í•œ ì„ íƒì)
            content = ""
            content_selectors = [
                '.se-main-container',
                '.se_component_wrap', 
                '#postViewArea',
                '.se-viewer',
                '.se_doc_viewer',
                '[class*="se-"]',
                '.post_ct',
                '.blog_content'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ (íƒœê·¸ ì œê±°)
                    content = content_elem.get_text(separator='\n', strip=True)
                    if len(content) > 100:  # ì¶©ë¶„í•œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì‚¬ìš©
                        break
            
            # ì—¬ì „íˆ ë‚´ìš©ì´ ë¶€ì¡±í•˜ë©´ ì „ì²´ bodyì—ì„œ ì¶”ì¶œ
            if len(content) < 100:
                body = soup.find('body')
                if body:
                    content = body.get_text(separator='\n', strip=True)
            
            print(f"ğŸ“ ì¶”ì¶œëœ ë‚´ìš© ê¸¸ì´: {len(content)}ì")
            
            return self.clean_text({
                'title': title or "ì œëª© ì—†ìŒ",
                'content': content,
                'platform': 'naver',
                'url': url
            })
            
        except Exception as e:
            return {'error': f"Selenium ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {str(e)}"}
    
    def scrape_blog(self, url):
        """í†µí•© ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í•‘ (Selenium ìš°ì„ )"""
        if not self.is_supported_blog(url):
            return {'error': 'ì§€ì›ë˜ì§€ ì•ŠëŠ” ë¸”ë¡œê·¸ í”Œë«í¼ì…ë‹ˆë‹¤.'}
        
        domain = urlparse(url).netloc.lower()
        
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” Selenium ì‚¬ìš©
        if 'blog.naver.com' in domain:
            if self.use_selenium:
                result = self.scrape_naver_blog_selenium(url)
                # Selenium ê²°ê³¼ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ì¼ë°˜ ìŠ¤í¬ë˜í•‘ë„ ì‹œë„
                if 'error' not in result and result.get('content_length', 0) > 100:
                    return result
                else:
                    print("âš ï¸ Selenium ê²°ê³¼ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì¼ë°˜ ìŠ¤í¬ë˜í•‘ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            
            return self.scrape_naver_blog_fallback(url)
        
        elif 'tistory.com' in domain:
            return self.scrape_tistory_blog(url)
        else:
            return self.general_scrape(url)
    
    def scrape_naver_blog_fallback(self, url):
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì¼ë°˜ ìŠ¤í¬ë˜í•‘ (fallback)"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë” ë§ì€ ì„ íƒì ì‹œë„
            title = ""
            content = ""
            
            # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ìˆëŠ” ë¶€ë¶„ ì¶”ì¶œ
            all_text = soup.get_text()
            lines = [line.strip() for line in all_text.split('\n') if line.strip()]
            
            # ê¸´ ì¤„ë“¤ì„ contentë¡œ ì‚¬ìš© (ê´‘ê³ ë‚˜ ë©”ë‰´ê°€ ì•„ë‹Œ)
            content_lines = []
            for line in lines:
                if len(line) > 10 and not any(keyword in line.lower() for keyword in 
                    ['copyright', 'ì €ì‘ê¶Œ', 'naver', 'ë„¤ì´ë²„', 'menu', 'ë©”ë‰´', 'login', 'ë¡œê·¸ì¸']):
                    content_lines.append(line)
            
            content = '\n'.join(content_lines[:50])  # ìƒìœ„ 50ì¤„ë§Œ
            
            return self.clean_text({
                'title': title or "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨",
                'content': content,
                'platform': 'naver_fallback',
                'url': url
            })
            
        except Exception as e:
            return {'error': f"ì¼ë°˜ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {str(e)}"}
    
    def is_supported_blog(self, url):
        """ì§€ì›ë˜ëŠ” ë¸”ë¡œê·¸ í”Œë«í¼ì¸ì§€ í™•ì¸"""
        domain = urlparse(url).netloc.lower()
        supported_domains = [
            'blog.naver.com',
            'tistory.com',
            'velog.io',
            'brunch.co.kr'
        ]
        return any(supported in domain for supported in supported_domains)
    
    def scrape_tistory_blog(self, url):
        """í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í•‘ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            title = ""
            title_selectors = ['.title', '.entry-title', 'h1', 'h2', '.post-title']
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text().strip()
                    break
            
            content = ""
            content_selectors = ['.entry-content', '.post-content', '.article', '.content']
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem.get_text()
                    break
            
            return self.clean_text({
                'title': title,
                'content': content,
                'platform': 'tistory',
                'url': url
            })
            
        except Exception as e:
            return {'error': f"í‹°ìŠ¤í† ë¦¬ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {str(e)}"}
    
    def general_scrape(self, url):
        """ì¼ë°˜ ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            title = soup.title.string if soup.title else ""
            
            content_selectors = ['article', 'main', '[class*="content"]', '[class*="post"]']
            content = ""
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem.get_text()
                    break
            
            if not content:
                content = soup.get_text()
            
            return self.clean_text({
                'title': title,
                'content': content,
                'platform': 'general',
                'url': url
            })
            
        except Exception as e:
            return {'error': f"ì¼ë°˜ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {str(e)}"}
    
    def clean_text(self, data):
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if 'error' in data:
            return data
        
        title = re.sub(r'\s+', ' ', data['title']).strip()
        
        content = data['content']
        content = re.sub(r'\n\s*\n', '\n', content)
        content = re.sub(r'\s+', ' ', content)
        content = content.strip()
        
        # ë„ˆë¬´ ê¸´ ê²½ìš° ìë¥´ê¸°
        if len(content) > 5000:
            content = content[:5000] + "..."
        
        return {
            'title': title,
            'content': content,
            'platform': data['platform'],
            'url': data['url'],
            'content_length': len(content),
            'word_count': len(content.split())
        }
    
    def __del__(self):
        """ì›¹ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_scraper():
    """ê°œì„ ëœ ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸"""
    print("ğŸ”§ ê°œì„ ëœ ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸")
    
    scraper = BlogScraper(use_selenium=True)
    
    test_url = input("í…ŒìŠ¤íŠ¸í•  ë¸”ë¡œê·¸ URLì„ ì…ë ¥í•˜ì„¸ìš”: ")
    
    if test_url:
        print("ğŸ”„ Seleniumìœ¼ë¡œ ìŠ¤í¬ë˜í•‘ ì¤‘...")
        result = scraper.scrape_blog(test_url)
        
        if 'error' in result:
            print(f"âŒ ì˜¤ë¥˜: {result['error']}")
        else:
            print("âœ… ìŠ¤í¬ë˜í•‘ ì„±ê³µ!")
            print(f"ì œëª©: {result['title']}")
            print(f"í”Œë«í¼: {result['platform']}")
            print(f"ë‚´ìš© ê¸¸ì´: {result['content_length']}ì")
            print(f"ë‹¨ì–´ ìˆ˜: {result['word_count']}ê°œ")
            print(f"\në‚´ìš© ë¯¸ë¦¬ë³´ê¸°:\n{result['content'][:500]}...")

if __name__ == "__main__":
    test_scraper()